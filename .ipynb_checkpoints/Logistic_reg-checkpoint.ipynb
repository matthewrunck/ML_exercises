{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Trains a logisitic regression classifier on the Madelon and Gisette datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from Load_data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training and test data\n",
    "gis_train,gis_train_labels,gis_test,gis_test_labels=load_data(\"Data\\\\gisette_train.data\",\"Data\\\\gisette_train.labels\",\"Data\\\\gisette_valid.data\",\"Data\\\\gisette_valid.labels\")\n",
    "mad_train,mad_train_labels,mad_test,mad_test_labels = load_data(\"Data\\\\madelon_train.data\",\"Data\\\\madelon_train.labels\",\"Data\\\\madelon_valid.data\",\"Data\\\\madelon_valid.labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 4956)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gis_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 501)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mad_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logisitc Regression by Gradient Ascent\n",
    "The model for logistic regression is $log\\frac{p(x)}{1-p(x)}=\\beta x^T$ where $x$ is the vector of values of explanatory variables and $\\beta$ is a vector of weights for each variable. Solvng for $p$ gives $p(x)=\\frac{1}{1+e^{-\\beta x^T}}$. This is also known as the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(beta,X):\n",
    "        sig = 1/(1+np.exp(-np.dot(X,beta)))\n",
    "        return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood function for the logisitc model with n observations is \n",
    "$$L(\\beta)=\\sum_{i=1}^n (y_i*\\beta x_i^T-log(1+e^{\\beta x_i^T})$$ where $y_1$ is the outcome of the $i-th$ observation with value 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #Likeihood function for logistic model for current weights (w)\n",
    "def Like(beta,X,y):\n",
    "    betax = np.dot(X,beta)\n",
    "    return (np.dot(y.T,betax)-np.sum(np.log(1+np.exp(betax))))[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to maximize the likelihood function, find the gradient and solve for zero (since the log-likelihood is concave, this will give the unique maximum). The gradient is $$\\nabla L(\\beta) = \\sum_{i=1}^n(y_i-\\frac{1}{1+e^{-\\beta x_i^T}})x_i^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Return the gradient of the likelhihood funtion\n",
    "def dL(beta,X,y):\n",
    "    delt = np.dot((y-sigmoid(beta,X)).T,X)\n",
    "    return delt.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Ascent\n",
    "There is no closed form solution to the equation $\\nabla L(\\beta)$. Numerical methods must be used instead. The first approach we will employ is gradient ascent. This is an iterative appraoch that updates the weights by moving them in the direction of the gradient (since the gradient will always point towards the global maximum.\n",
    "$$ \\beta_{t+1} = \\beta_t + \\eta \\nabla L(\\beta)$$\n",
    "where $\\eta$ is a hyperparamter known as the learning rate that determines how much $\\beta$ changes with each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_asc(X,y,eta=1,max_iter=100):\n",
    "    n,m = X.shape\n",
    "    beta =np.zeros((m,1)) # initializate weights\n",
    "    LL = Like(beta,X,y) #Log-likelihood with initial weights\n",
    "    LL_seq =[LL]\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        beta_star = beta + eta*dL(beta,X,y)/len(y) #Calculate new weights\n",
    "        LL_new = Like(beta_star,X,y) #Calculate log-likelihood with new weights\n",
    "        LL_seq+=[LL_new]\n",
    "        delta = abs(LL_seq[-2] - LL_seq[-1]) #Change in log-likelihood with new weights\n",
    "        beta = beta_star #Update weights\n",
    "        \n",
    "        #Check for convergence of likelihood and stop if convergence achieved\n",
    "        if delta < 1e-6:\n",
    "            print(\"Convergence reached after\",i+1,\"iterations\")\n",
    "            return({'Estimate':beta_star,'Iter':i,'LogLike':LL_seq})\n",
    "\n",
    "    print(\"Maximum iteration reached without convergence\")\n",
    "    return({'Estimate':beta_star,'Iter':i,'LogLike':LL_seq})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton Raphson via IRLS\n",
    "Gradient ascent can take many iterations to converge, and relies on picking a good learning rate to do so. \n",
    "Usually, faster convergence can be achieved using the Newton Raphson method where we use the Hessian matrix of second order derivatives of $L(\\beta)$.\n",
    "In matrix notation, let $X$ be a $n \\times m$ matrix of $x_i$ observations, $y$ is a vector of responses and $p$ is the vector of fitted probabilities with $i-th$ element  $p_i = \\frac{1}{1+e^{-\\beta x_i^T}}$, and $W$ be a $ n\\times n$ mdiagonal matrix with of weights with $i-th$ diagonal element $p_i(1-p_i)$. Then we have:\n",
    "\n",
    "$$\\nabla L(\\beta) = X^T(y-p)$$\n",
    "$$H(L(\\beta) = -X^TWX$$\n",
    "\n",
    "So to update the weights the Newton step is \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\beta^{(t+1)}& = \\beta^{(t)} + (X^TW^{(t)}X)^{-1}X^T(y-p^{(t)})\\\\\n",
    "& = (X^TW^{(t)}X)^{-1}X^TW^{(t)}(X^T \\beta^{(t)}+ W^{(t)-1}(y-p^{(t)})\\\\\n",
    "& = (X^TW^{(t)}X)^{-1}X^TW^{(t)}z^{(t)}\n",
    "\\end{align}\n",
    "$$\n",
    "where $z^{(t)} = X^T \\beta^{(t)}+ W^{(t)-1}(y-p^{(t)})$ is called the adjusted response.\n",
    "This proceduce is known as *Iteratively reweighted least squares* or IRLS since each update involves solving a weighted least squares problem $\\beta^{(t+1)} = argmin_{\\beta}(z^{(t)}-X\\beta)^TW(z^{(t)}-X\\beta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IRLS(X,y,max_iter=10):\n",
    "    n,m = X.shape\n",
    "    beta =np.zeros((m,1)) # initializate weights\n",
    "    LL = Like(beta,X,y) #Log-likelihood with initial weights\n",
    "    LL_seq =[LL]\n",
    "    for i in range(max_iter):\n",
    "\n",
    "        h = X.dot(beta)\n",
    "        pi = 1/(1+np.exp(-h))\n",
    "        pi_adj = pi\n",
    "        pi_adj[pi_adj==1.0] = 0.99999999\n",
    "        pi_adj[pi_adj==0.0] = 0.00000001\n",
    "        s =pi_adj*(1-pi_adj)\n",
    "                     # Evaluate the probabilities\n",
    "        weight=np.diag(np.ravel(s)) # Set the diagonal\n",
    "        # calculate the working response vector, avoiding division by zero\n",
    "        z = np.array(X.dot(beta) + np.divide((y-pi_adj),s),dtype=np.float32)\n",
    "\n",
    "        # calculate the new coefficients\n",
    "        XtWX = ((X.T).dot(weight)).dot(X)\n",
    "        inv_XtWX = np.linalg.pinv(XtWX)\n",
    "        \n",
    "        XtWz = (X.T).dot(weight.dot(z))\n",
    "        beta_star = inv_XtWX.dot(XtWz)\n",
    "        LL_star = Like(beta_star,X,y)\n",
    "        LL_seq +=[LL_star]\n",
    "\n",
    "        #Update w\n",
    "        beta = beta_star\n",
    "        # Check for convergence\n",
    "        delta = abs(LL_seq[-2] - LL_seq[-1])\n",
    "        if delta < 1e-6:\n",
    "            print(\"Convergence reached after\",i+1,\"iterations\")\n",
    "            return({'Estimate':beta_star,'Iter':i,'LogLike':LL_seq})\n",
    "        # If the convergence criterion is not satisfied, continue\n",
    "    print(\"Maximum iteration reached without convergence\")\n",
    "    return({'Estimate':w_star,'Iter':i,'LogLike':LL_seq})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogReg(X,y,eta=1.8,max_iter=1000, method='IRLS'):\n",
    "    \n",
    "    if method not in ['IRLS','grad_ascent']:\n",
    "        print('invalid method (must be IRLS or grad_ascent)')\n",
    "        return None\n",
    "        \n",
    "    \n",
    "    elif method=='grad_ascent':\n",
    "        out=grad_asc(X,y,eta=eta,max_iter=max_iter)\n",
    "          \n",
    "    \n",
    "    else:\n",
    "        out = IRLS(X,y,max_iter=max_iter)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 5 iterations\n",
      "Time elapsed: 3.1572184562683105s\n"
     ]
    }
   ],
   "source": [
    "start=time.time()    \n",
    "w_mad_IRLS = LogReg(mad_train,mad_train_labels,method='IRLS',max_iter=10)\n",
    "stop=time.time()\n",
    "madelon_IRLS_time=stop-start\n",
    "print('Time elapsed: '+str(madelon_IRLS_time)+'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence reached after 4877 iterations\n",
      "Time elapsed: 22.522571325302124s\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "w_mad_grad=LogReg(mad_train,mad_train_labels,eta=0.85,method='grad_ascent',max_iter=10000)\n",
    "stop=time.time()\n",
    "madelon_grad_asc_time=stop-start\n",
    "print('Time elapsed: '+str(madelon_grad_asc_time)+'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-7c67cc39de12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mw_gis_IRLS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogReg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgis_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgis_train_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'IRLS'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgisette_IRLS_time\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time elapsed: '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgisette_IRLS_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-98-51c216787931>\u001b[0m in \u001b[0;36mLogReg\u001b[1;34m(X, y, eta, max_iter, method)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIRLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-97-7b34788144f4>\u001b[0m in \u001b[0;36mIRLS\u001b[1;34m(X, y, max_iter)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# calculate the new coefficients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mXtWX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0minv_XtWX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpinv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtWX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start=time.time()    \n",
    "w_gis_IRLS = LogReg(gis_train,gis_train_labels,method='IRLS',max_iter=10)\n",
    "stop=time.time()\n",
    "gisette_IRLS_time=stop-start\n",
    "print('Time elapsed: '+str(gisette_IRLS_time)+'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()    \n",
    "w_gis_grad=LogReg(gis_train,gis_train_labels,eta=0.85,method='grad_ascent')\n",
    "stop=time.time()\n",
    "gisette_grad_asc_time=stop-start\n",
    "print('Time elapsed: '+str(gisette_grad_asc_time)+'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_errors(train,train_labels,test,test_labels,weights):\n",
    "    train_preds = np.dot(train,weights)<0\n",
    "    train_error = np.mean(train_preds==train_labels)\n",
    "    test_preds = np.dot(test,weights)<0\n",
    "    test_error = np.mean(test_preds==test_labels)\n",
    "    return {'Train error':train_error[0],'Test Error':test_error[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gis_IRLS_err=get_errors(gis_train,gis_train_labels,gis_test,gis_test_labels,w_gis_IRLS['Estimate'])\n",
    "gis_grad_err=get_errors(gis_train,gis_train_labels,gis_test,gis_test_labels,w_gis_grad['Estimate'])\n",
    "gis_err_tab = pd.DataFrame({'IRLS':gis_IRLS_err,'Gradient Ascent':gis_grad_err})\n",
    "print(gis_err_tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_IRLS_err=get_errors(mad_train,mad_train_labels,mad_test,mad_test_labels,w_mad_IRLS['Estimate'])\n",
    "mad_grad_err=get_errors(mad_train,mad_train_labels,mad_test,mad_test_labels,w_mad_grad['Estimate'])\n",
    "mad_err_tab = pd.DataFrame({'IRLS':mad_IRLS_err,'Gradient Ascent':mad_grad_err})\n",
    "print(mad_err_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()    \n",
    "w_mad_IRLS = LogReg(mad_train,mad_train_labels,method='IRLS',max_iter=10)\n",
    "stop=time.time()\n",
    "madelon_IRLS_time=stop-start\n",
    "print(madelon_IRLS_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(w_mad_IRLS['LogLike'][2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_mad_grad['LogLike'][200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_gis_IRLS['LogLike'][3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w_gis_grad['LogLike'][200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mad_IRLS_err=get_errors(mad_train,mad_train_labels,mad_test,mad_test_labels,w_mad_IRLS['Estimate'])\n",
    "mad_grad_err=get_errors(mad_train,mad_train_labels,mad_test,mad_test_labels,w_mad_grad['Estimate'])\n",
    "mad_err_tab = pd.DataFrame({'IRLS':mad_IRLS_err,'Gradient Ascent':mad_grad_err})\n",
    "print(mad_err_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
